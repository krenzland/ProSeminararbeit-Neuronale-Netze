
\section{Vom linearen Modell zum Neuronalen Netz}

Sehr simpel; lineares Modell als Input -> Output layer,
dann lineare Funktion als Hidden Layer -> Lineares Modell \\
(Vergleich; Bild Logistic Regression vs "Lineares Perceptron" )

\subsection{Das Perceptron als additatives lineares Modell}

Modell perceptron. \\Problem: OR geht, XOR nicht -> lineare seperierbarkeit

\subsection{Das Perceptron als einfaches ANN}

Verallgemeinerung zum ANN. \\
blabla mit drei Quellenangaben\cite{ietf-ipfix-protocol,snoeren2001hash,belenky2003ip}

\subsection{Das XOR-Problem: nicht linear separierbare Probleme}
Theorem: Single Layer kann alle Funktionen annähern.

\subsection{Die Lösung: Neuronale Netze}
Neuronale Netze sind eine verallgemeinerung von Perceptrons. Sie besitzen noch eine weitere Schicht Neuronen, die jedoch nicht nur lineare Transformationen durchführen, sondern zusätzlich noch eine nicht lineare. \\

Dadurch ist ein Fit nicht mehr analytisch einfach lösbar; das Training muss numerisch erfolgen.

\section{Neuronale Netze} %TODO: Absplitten

\subsection{Die Aktivierungsfunktion}
Nicht linear. \\
Sigmoide Form. \\
Besonders effizient: Nicht symmetrische Form. \\

Sigmoid Function; Numerical Optim., 
...

\subsection{Die Kostenfunktion}
Mean squared Errors(MSE) (euklidische Norm)
\begin{equation}
J(\theta) = \frac{1}{n} \sum_{i=1}^n \left (\hat{Y_i} - Y_i \right)^2
\end{equation}

oder negative Cross entropy (TODO: Quelle, richtige Formel)
\begin{equation}
    -\frac1N\sum_{n=1}^N\ \bigg[y_n  \log \hat y_n + (1 - y_n)  \log (1 - \hat y_n)\bigg]
\end{equation}

\subsection{Die Minimierung der Kostenfunktion - Backpropagation}

\subsection{Die numerische Methode - Gradient Descent und verwandte}

\section{Neuronale Netze in der Praxis}
Weigth Decay, Anfangwert

\subsection{Problematik von lokalen Minima}
Cost function hat mehrere Minima.
\\
Lösung: Neu starten
\\ 
Oft nicht globales minimum erwünscht (siehe Overfitting)

\subsection{Anzahl Hidden Layer}

\subsection{Initialisierung der Gewichte}
Randomisiert - oft nicht gut

\subsection{Vermeidung von Overfitting}
Weigth Decay (entspricht L2-Regularisierung)
\\Bayesian View of L2-Reg.
\subsection{Regression}

\subsection{Klassifizierung}
Cross entropy.
\\
Soft-Max-Function
\\
\subsection{...}
https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/
https://snippyhollow.github.io/blog/2014/08/09/so-you-wanna-try-deep-learning/

http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf Efficient Backprop
Zitat: Nonlinear activationut/ functions are what give neural networks their nonlinear capabilities. 

https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw

https://stats.stackexchange.com/questions/63152/what-does-the-hidden-layer-in-a-neural-network-compute
