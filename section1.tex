\section{Vom linearen Modell zum Neuronalen Netz}
Eines der klassischsten Modelle der Regressionsanalyse sind lineare Regression und die logistic Regression. \\

Es sind mathematisch simple Verfahren, die sogar analytisch lösbar sind - aber eben nicht immer ideale Ergebnisse erzielen: es gibt Probleme, die mit den Verfahren nicht lösbar sind. 
\\


Sehr simpel; lineares Modell als Input -> Output layer,
dann lineare Funktion als Hidden Layer -> Lineares Modell \\
(Vergleich; Bild Logistic Regression vs "Lineares Perceptron" )

\subsection{Das Perceptron als additatives lineares Modell}

Modell perceptron. \\

Es gibt viele Probleme, die von einem Perceptron nicht gelöst werden können. Es können nur Funktionen von linearen Modellen approximiert werden, die linear seperierbar sind. Da das Perceptron als Verknüfung von linearen Funktionen auch linear ist, kann auch es diese Klasse von Funktionen nicht lösen.

\subsection{Das Perceptron als einfaches ANN}

Verallgemeinerung zum ANN. \\
blabla mit drei Quellenangaben\cite{ietf-ipfix-protocol,snoeren2001hash,belenky2003ip}

\subsection{Das XOR-Problem: nicht linear separierbare Probleme}
Theorem: Single Layer kann alle Funktionen annähern.

\subsection{Die Lösung: Neuronale Netze}
Neuronale Netze sind eine verallgemeinerung von Perceptrons. Sie besitzen noch eine (oder mehrere) weitere Schicht(en) Neuronen, die jedoch nicht nur lineare Transformationen durchführen, sondern zusätzlich noch eine nicht lineare. \\

Diese nicht-linearität ermöglicht es, auch nicht linear seperierbare Probleme zu lösen.\\

Durch die Nicht-linearität und durch die komplexere Verknüpfung resultiert ein nicht lineares Optimierungsproblem, das meistens nicht mehr analytisch zu lösen ist: es werden numerische Verfahren benutzt.


