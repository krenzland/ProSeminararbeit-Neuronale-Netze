
\section{Vom linearen Modell zum Perceptron}

Sehr simpel; lineares Modell als Input -> Output layer,
dann lineare Funktion als Hidden Layer -> Lineares Modell \\
(Vergleich; Bild Logistic Regression vs "Lineares Perceptron" )

\subsection{Lineare Regression als einfaches graphisches Modell}

Modell perceptron. \\Problem: OR geht, XOR nicht -> lineare seperierbarkeit

\subsection{Das Perceptron als einfaches ANNs}

Verallgemeinerung zum ANN. \\
blabla mit drei Quellenangaben\cite{ietf-ipfix-protocol,snoeren2001hash,belenky2003ip}

\subsection{Motherfucking ANN}
Theorem: Single Layer kann alle Funktionen ann√§hern.

\section{Training} %TODO: Absplitten

\subsection{Backpropagation}
Sigmoid Function, Costfunction; Numerical Optim., 
...
\subsection{Vermeidung von Overfitting}

Weigth Decay, Anfangwert

\subsection{Anzahl Hidden Layer}

\section{Auswertung der Output Layer}

\subsection{Regression}

\subsection{Classification}

https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/
https://snippyhollow.github.io/blog/2014/08/09/so-you-wanna-try-deep-learning/

http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf Efficient Backprop
Zitat: Nonlinear activation functions are what give neural networks their nonlinear capabilities. 

https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw

https://stats.stackexchange.com/questions/63152/what-does-the-hidden-layer-in-a-neural-network-compute
