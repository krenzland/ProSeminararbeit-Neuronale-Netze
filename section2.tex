\section{Neuronale Netze} %TODO: Absplitten

\subsection{Aufbau und Namenskonvetionen}
\begin{figure}[ht!]
  \centering
    \input{section4}
  \caption{Ein 2-schichtiges MLP.}
\end{figure}


Ein Neuronales Netz besteht aus mehreren Schichten Neuronen. Einem Input Layer, einen (oder mehreren) Hidden Layer, und einem Output Layer. 
Wenn man von einem k-schichtigen ANN spricht, bezeichnet k die Anzahl der Schichten, ohne Input-Layer.

Sowohl Input, als auch Hidden-Layer besten aus Neuronen mit einer Aktivierungsfunktion, die die Eingangsdaten transformiert.

Jeder Layer ist mit den anderen Elementweise verbunden. Jede Verbindung hat ein so genanntes Gewicht. Daten fließen nur von links nach rechts. 

\todo{Unbeding folgenden Abschnitt bearbeiten, unleserlich, nicht aktuell und komplett sinnlos! (Aufbau Neuronaler Netze und Namenskonvetionen)}
Wir benutzen folgende Namenskonvention: $m$ ist die Anzahl der Parameter, $x \in \mathbb{R}^{m \times 1}$ ist der Input-Vektor, 
$w_i\in \mathbb{R}^{\text{Anzahl Neuronen} \times \text{Anzahl Neuronen Layer davor}}$ der Gewichtsvektor, der die $i$-te Schicht mit der $i+1$ Schicht verbindet; $b \in \mathbb{R}^{\text{Anzahl Neuronen} \times 1}$ der Bias-Vektor; $\hat{y}$ ist der Output des Netzwerkes, $y$ der Referenzwert (Ergebniss im Trainingsset).  

\subsection{Die Aktivierungsfunktion}
Die Funktion, die pro Knoten die nicht-lineare Transformation durchführt, heißt Aktivierungsfunktion. \\
Häufig wird eine Funktion mit sigmoiden Erscheinungsbild gewählt, das heißt eine Funktion, die die Ergebnisse in ein bestimmtes Interval "zusammenquetscht" (Quelle) und ein an ein "S" erinnerndes Erscheinungsbild hat. 
\\
Beispiele für Funktion diesen Typs sind die so genannte logistische Funktion

\begin{equation}
\sigma_1(x) = \frac{1}{1+e^{-x}}.
\end{equation}

und der hyperbolische Tangens

\begin{equation}
\sigma_2(x) = \tanh(x) = \frac{1-e^{-2x}}{1+e^{-2x}} = 
\sigma_1(2x) -1
\end{equation}.

Die logistische Funktion ist nicht 0-zentriert. Das hat den Effekt, dass, falls alle Eingangswerte des Neurons, der Gradient entweder komplett positiv, oder negativ ist. Das führt unter Umständen zu unerwünschten Oszillationen bei der Optimierung. Der hyperbolische Tangens ist 0-zentriert, hat aber einen Nachteil mit der logistischen Funtkion gemeinsam: sehr kleine oder sehr große Werte haben einen sehr kleinen Gradienten zur Folge; dies führt zum "Absterben" des Neurons. 
In \cite{lecunefficient} wird $1.7159 \tanh(\frac{2}{3})$ empfohlen, wenn eine Normalisierung vorgenommen wird. 

Eine alternative Aktivierungsfunktion ist die Rectifier-Funktion 

\begin{equation}
\sigma_3(x) = \max(0,x).
\end{equation} 

Glorot et al argumentieren, dass die logistische Funktion zwar biologisch plausibler ist als der hyperbolische Tangens, aber die Rectifier Funktion noch näher an der Funktionsweise biologischer Neuronen ist. Was für uns relevanter ist: sie ist effizienter zu berechnen; also in der Praxis oft eine bessere Wahl. \cite{glorot2011deep}

Problematisch ist bei der Rectifier-Funktion jedoch, dass durch $x < 0$ das Neuron abstirbt, das heißt der Gradient ist 0: Bei der Backpropagation kann ein Fehler nicht korrigiert werden. \cite{bengio2012practical}


\subsection{Die Kostenfunktion}
Wir benutzen eine Kostenfunktion, die den durchschnittlichen Fehler des ANN-Ergebnis quantifiziert.\\

Da neuronale Netze als Modellierung der Wahrscheinlichkeit der Vorhersage, dass ein Output Vektor $t$ für neue Eingabewerte $x$ angesehen werden, kann man den Fehler bezüglich der bedingten Wahrscheinlichkeit $p(t|x)$ darstellen. 
Um eine Fehlerfunktion zu bilden benutzt man nun die Maximum-Likehood-Funktion, die Parameter einer Wahrscheinlichkeitsverteilung schätzt. 

\begin{equation}
L = \prod_n p(x^n t^n) = \prod_n p(t^n|x^n) (x^n) 
\end{equation}

Anstatt die Maximum-Likehood-Funktion zu maximieren, minimieren wir die negative, logarithmische Maximum-Likehood-Funktion. Das funktioniert, weil $\ln$ eine monotone Funktion ist, es erleichtert uns außerdem mit großen Werten zu rechnen.

\begin{IEEEeqnarray}{rCl}
J & = & -\ln L 
\nonumber \\
& = & - \sum_n \ln p(t^n|x^n) - \sum_n \ln p(x^n)
\nonumber \\
& = & - \sum_n \ln p(t^n|x^n)
\end{IEEEeqnarray}

Aus dieser allgemeinen Fehlerfunktionen lassen sich spezielle Fehlerfunktionen ableiten, die in der Praxis benutzt werden \cite{bishop1995neural}:

Wir wollen jetzt wieder den Fehler für $p(t|k)$ darstellen. Wir nehmen an, dass die Variable $t_k$ aus einer deterministischen Funktion, und normalverteiltem Rauschen (mit dem arithmetischen Mittel $0$ und Varianz $\sigma$ gebildet wird.  
Deswegen gilt: 

\begin{equation}
  p(t_k|x) = \frac{1}{(2 \pi \sigma^2)^{0.5}} \exp (-\frac{ \{ y_k(x; w) - t_k \}^2 }{2 \sigma^2})
\end{equation}.

Den Fehlerterm können wir jetzt einfach als Summe über alle Fehlerwahrscheinlichkeiten darstellen:

\begin{equation}
  E = \frac{1}{2 \sigma^2} \sum_{n=1}^{N} \sum_{k=1}^{c} \{ y_k(x^n; w) - t_k^n \}^2 + Nc \ln \sigma + \frac{N_c}{2} \ln (2 \pi)
\end{equation}

Ignorieren wir nun alle Faktoren, die nicht von den Parametern des Netztes abhängen, erhalten wir 

\begin{equation}
J(\theta) = \sum_{k=1}^K \sum_{i=1}^N \left( \hat{Y_i} - Y_i \right)^2,
\end{equation}

was der Sum-of-squared-errors Fehlerfunktion entspricht.\cite{bishop1995neural}


Eine für Klassifizierungsprobleme oft benutze Fehlerfunktion ist die negative Kreuzentropie:

\begin{equation}
    J(\theta) = -\sum_{k=1}^K \sum_{i=1}^N y_{i_k} \ln f_k(x_i).
\end{equation}


\subsection{Feedforward}
Um ein ANN zur Vorhersage neuer Input-Vektoren zu benutzen, wird es im sogenannten Feedforward Modus betrieben. In diesem wird der Input Vektor zuerst den Neuronen des Input-Layers präsentiert. Alle anderen Neuronen des nächsten Layers bilden dann eine lineare Kombination aus den Neuronen ihrer Vorgängerschicht: 

\begin{equation}
\text{net}_j = \sum_{i=1}^d x_i w_{j,i} + w_{j,0} = \sum_{i=0} x_i w_{j,i} = 
w_j^t x.
\end{equation}

Nachdem diese lineare Transformation abgeschlossen ist, wird bei allen Neuronen (außer bei denen im Output-Layer) die Aktivierungsfunktion benutzt, die eine nicht-lineare Transformation durchführt: 

\begin{equation}
y_j = \sigma (\text{net}_j).
\end{equation}

Dieser Prozess wird so lange durchgeführt, bis die Daten das komplette Netz durchlaufen haben, und im Output-Layer ankommen.

\cite{duda2012pattern}

\subsection{Die Minimierung der Kostenfunktion - Backpropagation}

Natürlich reicht es für maschinelles Lernen nicht aus, nur Vorhersagen zu treffen - das Modell muss sich auch anpassen können. Bei Neuronalen Netzen heißt der benutzte Trainingsalgorithmus Backpropagation.

Es ist eine Möglichkeit, Fehler, die bei den Ausgabeneuronen auftreten auch für die Neuronen der Schichten davor zu berücksichtigen - der Fehler wird zurückgesendet.

Dazu wird als erster Schritt für einen Trainingsvektor ein Forward-Pass durchgeführt - damit wird die Vorhersage des Netzwerkes berechnet, und anschließend mit der vorgegeben, korrekten Lösung des Trainingssets verglichen.

Diese Werte werden dann benutzt, um den Fehler zu berechnen, und dann die Parameter(also die Gewichte) anzupassen.


Am Ende wird das Ergebniss der Optimierungsmethode benutzt, um die Gewichte zu aktualisieren. \cite{rumelhart1988learning}

Alle relevanten Fehlerfunktionen haben die Form 

\begin{equation}
J = \sum_n J_n(y_1, \ldots, y_c),
\end{equation},

das heißt, der Gesamtfehler ist die Summe aller Einzelfehler.
Wir nehmen weiterhin an, dass der Fehler als ableitbare Funktion der Output-Werte darstellbar ist.

Wir suchen jetzt die partielle Ableitung der Fehlerfunktion unter Berücksichtigung eines bestimmten Gewichts:

\begin{equation}
\frac{\partial J^n}{\partial w_{i,j}} = \frac{\partial J^n}{\partial net_j}  \frac{\partial net_j }{\partial w_{ji}}
\end{equation}

Wir definieren jetzt folgende Hilfsvarabeln:

\begin{IEEEeqnarray}{rCl}
\frac{\partial net_j }{\partial w_{ji}} & = & z_i 
\\
\frac{\partial J^n}{\partial net_j} & = & \delta_j  
\end{IEEEeqnarray}

Durch mehrfahres Anwenden der Kettenregel bekommen wir die so genannte Backpropagation-Formel:

\begin{equation}
\delta_j =  \begin{cases}
               \sigma ' (net_j) (y_j - \hat{y_j})           & \text{wenn j Ausgabeneuron ist}\\
               \sigma ' (net_j) \sum_k w_{kj} \delta_k     & \text{wenn j verdecktes Neuron ist}
           \end{cases} 
\end{equation} 

Dadurch können wir den Gradienten einfach rekursiv berechnen. \cite{bishop1995neural} \\
\todo{Backprop fertig schreiben!}

Wichtig ist noch die Unterscheidung zwischen:

\begin{LaTeXdescription}
	\item[Stochastic Backpropagation]
	Bei diesem Verfahren wird immer nur ein zufälliger Eingabevektor pro Iteration evaluiert, und dann sofort die Gewichte aktualisiert. 
	\item[Batch Backpropagation] 
	Bei der zweiten Methode werden alle Eingabevektoren des Trainingssets präsentiert und die Gewichtsänderungen summiert. Erst dann werden alle Gewichte aktualisiert. 
\end{LaTeXdescription}\cite{duda2012pattern}

Die stochastische Methode ist die in der Praxis bevorzugte, weil die Konvergenz nicht nur von der Große des Trainingsset, sondern vielmehr von der Anzahl der Iterationen und der Verteilung der Trainingsdaten - für große Sets ist ein Benutzen der Batch Methode nicht mehr sinnvoll. \cite{bengio2012practical}
Ein weiter Vorteil ist, dass sie redundante Trainingsdaten (die bei einem Training für Mustererkennung zwangsläufig auftreten) ausnutzt. Außerdem konveriert sie eventuell zu einem besseren Minimum als die Batch Methode. \cite{lecunefficient}
Wir werden hier primär die stochastische Methode betrachten, da sie in der Praxis die bessere ist. \cite{lecunefficient, bengio2012practical}


\subsection{Die numerische Methode - Gradient Descent und verwandte}

Der Gradient wird oft mit dem Gradientenverfahren, einer Methode um Funktionen zu minimieren, benutzt.

Das Gradientenverfahren lässt sich als Iterationsvorschrift darstellen:

\begin{equation}
x_{n+1}=x_n- \eta  \nabla F(x_n), 
\end{equation}

wobei $\nabla$ der Gradient ist; $\nabla_w$ ist der Gradient im Bezug auf die Gewichte. 
Die Konstante $\eta$ heißt Lernrate. Je höher sie ist, desto schneller konvergiert das Verfahren - bei einer zu hoch gewählten sind aber oft Oszillationen zu beobachten. 

Das Verfahren funktioniert, weil der negative Gradient in die Richtung zeigt, in der die Funktion am schnellsten kleiner wird. Daher kommt auch die Bezeichnung Verfahren des steilsten Abstiegs.\\


Auf ANNs angewant, sieht die Formel dann für jeden Zeitschritt $t$ folgendermaßen aus:

\begin{equation}
  \Delta_w^t = - \eta  \nabla_w J(w),
\end{equation}

beziehungsweise in Kombination mit der Backpropagationformel

\begin{equation}
\Delta w_x = - \eta  \delta_j  x_i.
\end{equation}

\begin{equation}
\Delta w_{j,i} = - \eta  \sum_n \delta_j^n  x_i^n 
\end{equation}

\cite{bishop1995neural} 

Man unterscheidet beim Training von ANNs zwischen Batch und Stochastic Gradient Descent.\\

Bei der Batch Methode werden alle Gewichte auf einmal aktualisiert, bei Stochastic GD nur einzelne, zufällig ausgewählte Gewichte.\\

Das Gradientenverfahren hat das Problem, dass es oft in lokalen Minima steckenbleibt - das ist vor allem bei ANNs problematisch, weil die gewählte Kostenfunktion in den meisten Fällen mehrere lokale Minima besitzt. Auch wenn es nicht unbedingt notwendig, oder sogar erwünscht ist, einen globalen Extremwert zu finden (vgl. Overfitting), existieren bessere numerische Verfahren. \\

Eine Verbesserung zum gewöhnlichen Gradientenverfahren ist es, einen so genannten Momentumparameter zu benutzen, der auch vorherige Ergebnisse einbezieht, und somit das oft beobachtete oszillierende Verhalten des Gradientenverfahrens behebt:

\begin{equation}
 \Delta_w^t = - \eta  \nabla_w J(w) + \gamma \Delta_w^{t-1}.
\end{equation}

Bei einigen Problen kann das Verfahren Vorteile bringen, oft ist aber SGD mit einer guten Wahl der Lernrate bereits optimal \cite{bengio2012practical}.
