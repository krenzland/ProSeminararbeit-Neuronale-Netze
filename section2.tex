\section{Neuronale Netze} %TODO: Absplitten

\subsection{Aufbau und Namenskonvetionen}
\begin{figure}[ht!]
  \centering
    \input{section4}
  \caption{Ein 2-schichtiges MLP.}
\end{figure}


Ein Neuronales Netz besteht aus mehreren Schichten Neuronen. Einem Input Layer, einen (oder mehreren) Hidden Layer, und einem Output Layer. 
Wenn man von einem k-schichtigen ANN spricht, bezeichnet k die Anzahl der Schichten, ohne Input-Layer.

Sowohl Input, als auch Hidden-Layer besten aus Neuronen mit einer Aktivierungsfunktion, die die Eingangsdaten transformiert.

Jeder Layer ist mit den anderen Elementweise verbunden. Jede Verbindung hat ein so genanntes Gewicht. Daten fließen nur von links nach rechts. 

Wir benutzen folgende Namenskonvention: $m$ ist die Anzahl der Parameter, $x \in \mathbb{R}^{m \times 1}$ ist der Input-Vektor, 
$w_i\in \mathbb{R}^{\text{Anzahl Neuronen} \times \text{Anzahl Neuronen Layer davor}}$ der Gewichtsvektor, der die $i$-te Schicht mit der $i+1$ Schicht verbindet; $b \in \mathbb{R}^{\text{Anzahl Neuronen} \times 1}$ der Bias-Vektor; $\hat{y}$ ist der Output des Netzwerkes, $y$ der Referenzwert (Ergebniss im Trainingsset).  

\subsection{Die Aktivierungsfunktion}
Die Funktion, die pro Knoten die nicht-lineare Transformation durchführt, heißt Aktivierungsfunktion. \\
Häufig wird eine Funktion mit sigmoiden Erscheinungsbild gewählt, das heißt eine Funktion, die die Ergebnisse in ein bestimmtes Interval "zusammenquetscht" (Quelle) und ein an ein "S" erinnerndes Erscheinungsbild hat. 
\\
Die einfachste Funktion diesen Typs ist die so genannte lnistische Funktion

\begin{equation}
\sigma_1(x) = \frac{1}{1+e^{-x}}
\end{equation}

oder der hyperbolische Tangens

\begin{equation}
\sigma_2(x) = \tanh(x) = \frac{1-e^{-2x}}{1+e^{-2x}} = 
\sigma_1(2x) -1
\end{equation}.

Die lnistische Funktion ist nicht 0-zentriert. Das hat den Effekt, dass, falls alle Eingangswerte des Neurons, der Gradient entweder komplett positiv, oder negativ ist. Das führt unter Umständen zu unerwünschten Oszillationen bei der Optimierung. Der hyperbolische Tangens ist 0-zentriert, hat aber einen Nachteil mit der lnistischen Funtkion gemeinsam: sehr kleine oder sehr große Werte haben einen sehr kleinen Gradienten zur Folge; dies führt zum "Absterben" des Neurons. 

Eine alternative Aktivierungsfunktion ist die Rectifier-Funktion 

\begin{equation}
\sigma_3(x) = \max(0,x).
\end{equation} 

Glorot et al argumentieren, dass die lnistische Funktion zwar biolnisch plausibler ist als der hyperbolische Tangens, aber die Rectifier Funktion noch näher an der Funktionsweise biolnischer Neuronen ist. Was für uns relevanter ist: sie ist effizienter zu berechnen; also in der Praxis oft eine bessere Wahl. \cite{glorot2011deep}


\subsection{Die Kostenfunktion}
Wir benutzen eine Kostenfunktion, die den durchschnittlichen Fehler des ANN-Ergebnis quantifiziert.\\

Da neuronale Netze als Modellierung der Wahrscheinlichkeit der Vorhersage, dass ein Output Vektor $t$ für neue Eingabewerte $x$ angesehen werden, kann man den Fehler bezüglich der bedingten Wahrscheinlichkeit $p(t|x)$ darstellen. 
Um eine Fehlerfunktion zu bilden benutzt man nun die Maximum-Likehood-Funktion, die Parameter einer Wahrscheinlichkeitsverteilung schätzt. 

\begin{equation}
L = \prod_n p(x^n t^n) = \prod_n p(t^n|x^n) (x^n) 
\end{equation}

Anstatt die Maximum-Likehood-Funktion zu maximieren, minimieren wir die negative, logarithmische Maximum-Likehood-Funktion. Das funktioniert, weil $\ln$ eine monotone Funktion ist, es erleichtert uns außerdem mit großen Werten zu rechnen.

\begin{IEEEeqnarray}{rCl}
J & = & -\ln L 
\nonumber \\
& = & - \sum_n \ln p(t^n|x^n) - \sum_n \ln p(x^n)
\nonumber \\
& = & - \sum_n \ln p(t^n|x^n)
\end{IEEEeqnarray}

Aus dieser allgemeinen Fehlerfunktionen lassen sich spezielle Fehlerfunktionen ableiten, die in der Praxis benutzt werden \cite{bishop1995neural}:

Wir wollen jetzt wieder den Fehler für $p(t|k)$ darstellen. Wir nehmen an, dass die Variable $t_k$ aus einer deterministischen Funktion, und normalverteiltem Rauschen (mit dem arithmetischen Mittel $0$ und Varianz $\sigma$ gebildet wird.  
Deswegen gilt: 

\begin{equation}
  p(t_k|x) = \frac{1}{(2 \pi \sigma^2)^{0.5}} \exp (-\frac{ \{ y_k(x; w) - t_k \}^2 }{2 \sigma^2})
\end{equation}.

Den Fehlerterm können wir jetzt einfach als Summe über alle Fehlerwahrscheinlichkeiten darstellen:

\begin{equation}
  E = \frac{1}{2 \sigma^2} \sum_{n=1}^{N} \sum_{k=1}^{c} \{ y_k(x^n; w) - t_k^n \}^2 + Nc \ln \sigma + \frac{N_c}{2} \ln (2 \pi)
\end{equation}

Ignorieren wir nun alle Faktoren, die nicht von den Parametern des Netztes abhängen, erhalten wir 

\begin{equation}
J(\theta) = \sum_{k=1}^K \sum_{i=1}^N \left( \hat{Y_i} - Y_i \right)^2,
\end{equation}

was der Sum-of-squared-errors Fehlerfunktion entspricht.\cite{bishop1995neural}


\begin{equation}
J(\theta) = \sum_{k=1}^K \sum_{i=1}^N \left( \hat{Y_i} - Y_i \right)^2
\end{equation}

$\hat{Y_I}$ entspricht dem Ergniss des ANN, $Y$ dem richtigen Ergebniss. \\

Bei Klasssifizierungsproblemen wird üblicherweise die Sum-of-squared-errors Fehlerfunktion, oder aber die negative Kreuzentropie benutzt \cite{Hastie2009}:

\begin{equation}
    J(\theta) = -\sum_{k=1}^K \sum_{i=1}^N y_{i_k} \ln f_k(x_i)
\end{equation}

Bei Klassifikationsproblem ist Cross entropy fast immer die bessere Wahl. \todo{Warum ist Cross entropy besser?}


\subsection{Feedforward}
Um ein ANN zur Vorhersage neuer Input-Vektoren zu benutzen, wird es im sogenannten Feedforward Modus betrieben. In diesem wird der Input Vektor zuerst den Neuronen des Input-Layers präsentiert. Alle anderen Neuronen des nächsten Layers bilden dann eine lineare Kombination aus den Neuronen ihrer Vorgängerschicht: 

\begin{equation}
\text{net}_j = \sum_{i=1}^d x_i w_{j,i} + w_{j,0} = \sum_{i=0} x_i w_{j,i} = 
w_j^t x.
\end{equation}

Nachdem diese lineare Transformation abgeschlossen ist, wird bei allen Neuronen (außer bei denen im Output-Layer) die Aktivierungsfunktion benutzt, die eine nicht-lineare Transformation durchführt: 

\begin{equation}
y_j = \sigma (\text{net}_j).
\end{equation}

Dieser Prozess wird so lange durchgeführt, bis die Daten das komplette Netz durchlaufen haben, und im Output-Layer ankommen.

\cite{duda2012pattern}

\subsection{Die Minimierung der Kostenfunktion - Backpropagation}

Natürlich reicht es für maschinelles Lernen nicht aus, nur Vorhersagen zu treffen - das Modell muss sich auch anpassen können. Bei Neuronalen Netzen heißt der benutzte Trainingsalgorithmus Backpropagation.

Es ist eine Möglichkeit, Fehler, die bei den Ausgabeneuronen auftreten auch für die Neuronen der Schichten davor zu berücksichtigen - der Fehler wird zurückgesendet.

Dazu wird als erster Schritt für einen Trainingsvektor ein Forward-Pass durchgeführt - damit wird die Vorhersage des Netzwerkes berechnet, und anschließend mit der vorgegeben, korrekten Lösung des Trainingssets verglichen.

Diese Werte werden dann benutzt, um den Fehler zu berechnen, und dann die Parameter(also die Gewichte) anzupassen.


Am Ende wird das Ergebniss der Optimierungsmethode benutzt, um die Gewichte zu aktualisieren. \cite{rumelhart1988learning}

Alle relevanten Fehlerfunktionen haben die Form 

\begin{equation}
J = \sum_n J_n(y_1, \ldots, y_c),
\end{equation},

das heißt, der Gesamtfehler ist die Summe aller Einzelfehler.
Wir nehmen weiterhin an, dass der Fehler als ableitbare Funktion der Output-Werte darstellbar ist.

Wir suchen jetzt die partielle Ableitung der Fehlerfunktion unter Berücksichtigung eines bestimmten Gewichts:

\begin{equation}
\frac{\partial J}{\partial w_{i,j}} = \frac{\partial J^n}{\partial a_j}  \frac{\partial a_j }{\partial w_{i,j}}
\end{equation}

Durch mehrfahres Anwenden der Kettenregel bekommen wir die so genannte Backpropagation-Formel:

\begin{equation}
\theta_j =  \begin{cases}
               \sigma ' (net_j) (y_j - \hat{y_j})           & \text{wenn j Ausgabeneuron ist}\\
               \sigma ' (net_j) \sum_k w_{k,j} \theta_k     & \text{j hidden Layer}
           \end{cases} 
\end{equation} 

Dadurch können wir den Gradienten einfach rekursiv berechnen. \cite{bishop1995neural} \\

Wichtig ist noch die Unterscheidung zwischen:

\begin{LaTeXdescription}
	\item[Stochastic Backpropagation]
	Bei diesem Verfahren wird immer nur ein zufälliger Eingabevektor evaluiert, und dann werden sofort die Gewichte aktualisiert. 
	\item[Batch Backpropagation] 
	Bei der zweiten Methode werden alle Eingabevektoren des Trainingssets präsentiert und die Gewichtsänderungen summiert. Erst dann werden alle Gewichte aktualisiert. 
\end{LaTeXdescription}\cite{duda2012pattern}

Die stochastische Methode ist die in der Praxis bevorzugte, weil die Konvergenz nicht nur von der Große des Trainingsset, sondern vielmehr von der Anzahl der Iterationen und der Verteilung der Trainingsdaten - für große Sets ist ein Benutzen der Batch Methode nicht mehr sinnvoll. \cite{bengio2012practical}  


\subsection{Die numerische Methode - Gradient Descent und verwandte}

Der Gradient wird oft mit dem Gradientenverfahren, einer Methode um Funktionen zu minimieren, benutzt.

Er ist relativ simpel:

\begin{equation}
x_{n+1}=x_n-\gamma_n \nabla F(x_n)
\end{equation}

Das Verfahren funktioniert, weil der negative Gradient in die Richtung zeigt, in der die Funktion am schnellsten kleiner wird. Daher kommt auch die Bezeichnung Verfahren des steilsten Abstiegs.
Die Konstante $\nabla$ heißt Lernrate. Je höher Sie ist, desto schneller konvergiert das Verfahren - bei einer zu hoch gewählten sind aber oft Oszillationen zu beobachten. 

Auf ANNs angewant, sehen die Formeln dann folgendermaßen aus:

\begin{equation}
\Delta w_{j,i} = - \eta  \delta_j  x_i 
\end{equation}

\begin{equation}
\Delta w_{j,i} = - \eta  \sum_n \delta_j^n  x_i^n 
\end{equation}

\cite{bishop1995neural} 

Man unterscheidet beim Training von ANNs zwischen Batch und Stochastic Gradient Descent.\\

Bei der Batch Methode werden alle Gewichte auf einmal aktualisiert, bei Stochastic GD nur einzelne, zufällig ausgewählte Gewichte.\\

Das Gradientenverfahren hat das Problem, dass es oft in lokalen Minima steckenbleibt - das ist vor allem bei ANNs problematisch, weil die gewählte Kostenfunktion in den meisten Fällen mehrere lokale Minima besitzt. Auch wenn es nicht unbedingt notwendig, oder sogar erwünscht ist, einen globalen Extremwert zu finden (vgl. Overfitting), existieren bessere numerische Verfahren. \\

Eine Verbesserung zum gewöhnlichen Gradientenverfahren ist es, einen so genannten Momentumparameter zu benutzen, der auch vorherige Ergebnisse einbezieht, und somit das oft beobachtete oszillierende Verhalten des Gradientenverfahrens behebt.

\begin{equation}
 \mathbf{b} = \mathbf{a}-\gamma\nabla F(\mathbf{a}) + \text{Momentum}
\end{equation}