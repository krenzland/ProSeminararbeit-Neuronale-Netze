\section{Neuronale Netze in der Praxis}
Weigth Decay, Anfangwert

\subsection{Das Gradientenverfahren in der Praxis}
Beim Gradientenverfahren ist es wichtig eine sinnvolle Schrittweite zu verwenden. Ist sie kleiner, als die optimale, ist die benötigte Rechenzeit höher. Ist sie größer, kann das Verfahren divergieren. Eine gute Heuristik für die Wahl des Parameters ist, mit einem großen Wert zu starten, und bei Divergenz es mit einer dreimal kleineren Schrittweite erneut zu probieren.
Eine andere gute Optimierung ist, weder Batch noch Stochastic Backpropagation zu nutzen, sondern eine Mini-batch-Variante, bei der mehrere, aber nicht alle Eingabevektoren evaluiert werden vor jedem Gewichtsupdate. \cite{bengio2012practical}\\
Die Kostenfunktion hat oft mehrere lokale Minima - das Gradientenverfahren kann dann in einem solchen steckenbleiben, das heißt, es konvergiert unter Umständen nicht zum gesuchten globalen Minimum. Auch wenn das globale Minimum nicht immer erwünscht ist (vlg. early stopping), ist es unter Umständen doch besser, es zu finden.\\
Die einfachste Lösung wäre natürlich, ein anderes numerisches Verfahren zu benutzen. Doch andere Verfahren sind meistens komplizierter zu implementieren, und unter Umständen sogar langsamer als SGD. Wenn man beim SGD in einem Minimum stecken bleibt, bietet es sich an, einfach mit neu initialisierten Gewichtsparametern neuzustarten.

\subsection{Anzahl Hidden Layer}

\subsection{Initialisierung der Gewichte}
Die Initialisierung der Gewichte ist ein schwieriges Problem. Eine oft benutzte Heuristik ist 

\begin{equation}
	W_{ij} \sim U [ -\frac{1}{\sqrt{N}} , \frac{1}{\sqrt{N}} ],
\end{equation}

wobei $U[-a, a]$ gleichverteilten Werten zwischen $-a$ und $a$ entspricht, und $n$ der Anzahl der Neuronen in der vorherigen Schicht. 

Ein Vorschlag, um dieses Problem zu umgehen ist die normalisierte Initialisierung

\begin{equation}
	W \sim U [ - \frac{\sqrt{6}}{\sqrt{n_j+n_{j+1}}} 
	, 			 \frac{\sqrt{6}}{\sqrt{n_j+n_{j+1}}} ]. 
\end{equation}

In der Praxis ergeben sich durch sie bessere Ergebnisse. \cite{glorot2010understanding}

\subsection{Vermeidung von Overfitting}
Ein Problem, dass in der Praxis auftreten kann, ist das so genannte Overfitting, das ist die übermäßige Anpassung an die Trainingsmenge. Oft ist das Modell zu stark angepasst und daher nicht mehr allgemein effizient.\\

Eine mögliche Lösung ist die L1/L2-Regularisierung. Bei ihr wird an die Kostenfunktion ein weiterer Term angehängt:\\

\begin{equation}
	J_{\text{L1}} = J + \lambda \sum_i \theta_i 
\end{equation}

\begin{equation}
	J_{\text{L2}} = J + \lambda \sum_i \theta_i^2
\end{equation}


Er skaliert mit der Summe aller Gewichte, es werden also komplexe Modelle bestraft. Der Name Weigth-Decay kommt daher, dass die Gewichte gegen 0 gehen. \cite{Hastie2009} % muss schöner gehen!
\\

\todo{ Ja, genau. Die L2 ist halt direkt als Gaussian-prior interpretierbar. 
Zusammen mit der per NLL-Fehlerfunktion als Likelihood entspricht 
das dann eben einem Maximum A Posteriori-Ansatz. 
Das macht das ganze dann zB auch mit Gaussian Processes vergleichbar 
und bettet das halt einfach in ein solides stochastisches Framework ein. }

Aus der Bayesianischen Perspektive betrachtet, ist eine L2-Regulierung gleichbedeutend damit, dass wir eine A-priori-Verteilung bezüglich der Parameter annehmen. Die Fehlerfunktion entspricht dann:

\begin{equation}
	J(z, \theta) = - \ln P (z | \theta).
\end{equation}

\cite{bengio2012practical} \\
Eine weitere Lösung ist das so genannte early stopping, bei dem man nicht mit der Backpropagation aufhört, wenn ein (lokales) Minimum gefunden wurde, sondern dann, wenn die Performance des Modells bei dem Validierungsset optimal ist. Dabei wird oft die Iteration oft beendet, wenn bei der Fehlerfunktion gar kein Minimum vorliegt.  \todo{Quelle early stopping!}

Dabei entspricht die L2-Regularisierung early stopping, es sollten also nicht beide Methoden gleichzeitig genutzt werden; L1-Regularisierung kann jedoch mit beiden Regularisierungsarten kombiniert werden. \cite{bengio2012practical}

\subsection{Regression}

\subsection{Klassifizierung}
Cross entropy.
\\
Soft-Max-F	unction
\\
% \subsection{...}
% https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/
% https://snippyhollow.github.io/blog/2014/08/09/so-you-wanna-try-deep-learning/

% http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf Efficient Backprop
% Zitat: Nonlinear activationut/ functions are what give neural networks their nonlinear capabilities. 

% https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw

% https://stats.stackexchange.com/questions/63152/what-does-the-hidden-layer-in-a-neural-network-compute
